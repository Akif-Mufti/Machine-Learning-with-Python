{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningML20NLML.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akif-Mufti/Machine-Learning-with-Python/blob/master/LearningML20NLML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi4JPe0AGhbC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a14e02fd-8a45-4799-b284-7b6e1068da3d"
      },
      "source": [
        "# K-Nearest Neighbors\n",
        "# The k-Nearest Neighbors algorithm (or KNN) locates the k most similar instances in the\n",
        "# training dataset for a new data instance. From the k neighbors, a mean or median output\n",
        "# variable is taken as the prediction. Of note is the distance metric used (the metric argument).\n",
        "# The Minkowski distance is used by default, which is a generalization of both the Euclidean\n",
        "# distance (used when all inputs have the same scale) and Manhattan distance (for when the\n",
        "# scales of the input variables di\u000ber). You can construct a KNN model for regression using the\n",
        "# KNeighborsRegressor class5\n",
        "\n",
        "\n",
        "# KNN Regression\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
        "'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "kfold = KFold(n_splits=10, random_state=7)\n",
        "model = KNeighborsRegressor()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-107.28683898039215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIgHfGInHvT5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "144c2eea-feb7-42a5-8a87-4c4c33d7dfa7"
      },
      "source": [
        "# Classi\fcation and Regression Trees\n",
        "# Decision trees or the Classi\fcation and Regression Trees (CART as they are known) use the train-\n",
        "# ing data to select the best points to split the data in order to minimize a cost metric. The default\n",
        "# cost metric for regression decision trees is the mean squared error, speci\fed in the criterion\n",
        "# parameter. You can create a CART model for regression using the DecisionTreeRegressor\n",
        "# class\n",
        "\n",
        "# Decision Tree Regression\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
        "'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "kfold = KFold(n_splits=10, random_state=7)\n",
        "model = DecisionTreeRegressor()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-38.709456470588236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hItM3LtjKkhT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "c65a6ab5-90f6-40a7-9484-c3898b23aefd"
      },
      "source": [
        "# Support Vector Machines\n",
        "# Support Vector Machines (SVM) were developed for binary classi\fcation. The technique has\n",
        "# been extended for the prediction real-valued problems called Support Vector Regression (SVR).\n",
        "# Like the classi\fcation example, SVR is built upon the LIBSVM library. You can create an SVM\n",
        "# model for regression using the SVR class\n",
        "\n",
        "# SVM Regression\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVR\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
        "'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=10, random_state=7)\n",
        "model = SVR()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-91.04782433324428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}